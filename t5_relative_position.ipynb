{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Relative position embeddings according to T5 paper"
      ],
      "metadata": {
        "id": "AUV2q3HcshS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The part of the paper that deals with position embeddings:\n",
        "\n",
        "> Since self-attention is order-independent (i.e. it is an operation on sets), it is common to provide an explicit position signal to the Transformer. While the original Transformer used a sinusoidal position signal or learned position embeddings, it has recently become more common to use relative position embeddings (Shaw et al., 2018; Huang et al., 2018a). Instead of using a fixed embedding for each position, relative position embeddings produce a different learned embedding according to the offset between the “key” and “query” being compared in the self-attention mechanism. We use a simplified form of position embeddings where each “embedding” is simply a scalar that is added to the corresponding logit used for computing the attention weights. For efficiency, we also share the position embedding parameters across all layers in our model, though within a given layer each attention head uses a different learned position embedding. Typically, a fixed number of embeddings are learned, each corresponding to a range of possible key-query offsets. In this work, we use 32 embeddings for all of our models with ranges that increase in size logarithmically up to an offset of 128 beyond which we assign all relative positions to the same embedding. Note that a given layer is insensitive to relative position beyond 128 tokens, but subsequent layers can build a sensitivity to larger offsets by combining local information from previous layers.\n",
        "\n",
        "Well, their approach can seem a little confusing due to the use of algorithms, which is why you won't find many online hobbyists like me implementing this approach. Personally, I searched and only found the original implementation in TensorFlow.\n",
        "\n",
        "Let's make it as simple as possible:\n",
        "* They adopt the approach of (Shaw et al., 2018; Huang et al., 2018a). That is, they adopt the idea of relative positional representations by modifying the attention matrix. But what's new?\n",
        "* Instead of using high-dimensional embedding vectors to encode relative distances, they use scalar values. That is, instead of using a vector from d dimension to represent the relative distance -2, they use a scalar value. Therefore the relative distance -2 is given a scalar value to represent it. So the embeddings here are scalar values. Nice! this reduces spatial complexity.\n",
        "* They merely modify the attention matrix, and do not modify the value matrix as their predecessors did. This is sufficient, as the experiments showed, modifying the value matrix did not improve performance. And this another reduction for spatial complexity.\n",
        "* Instead of using 2k+1 embedding vectors, they only use 32 ones.\n",
        "* Their predecessors shared the embedding parameters across different attention heads and across all layers. Whereas here they share it across layers but use different parameters across heads. That is, embedding the first head in the first layer is the same as embedding the first head in the second layer. But the first head is different from the second. This allows each head to capture certain positional information that the rest may not.\n",
        "\n",
        "**Now how do things work?**\n",
        "1. First we have what is called `num_buckets` which indicates the number of embeddings (in our case it is 32), and we have another variable `max_distance`, in addition to a complex logarithmic function.\n",
        "2. Two separate equal sets of embeddings are allocated, one for close distances and one for farther distances. For example, in our case we have 32 embeddings, so 16 of them are reserved for encoding close distances and the other 16 are reserved for longer distances. For close distances, they give each relative distance its own unique embedding. Well this is similar to what their ancestors were doing (this is necessary for precise). As for long distances, a complex logarithmic function is used to sort the relative distances into groups (binning). For example, in our case, relative distances from 8 onwards (since it is bidirectional, i.e. there are relative distances from right and left, the number of embeddings from each direction will be 8) are sorted into groups, each group having a shared embedding. For example, relative distances from 8 to 12 have a shared one, relative distances from 12 to 16 have another one, and so on until we reach the maximum relative distance `max_distance`. Starting from `max_distance` all relative distances are the same as the last set. Of course, the number of elements in each group increases as we move away from the center. For example, the number of relative distances in the first group may be 4, while the third group may have 7, and so on. This approach may be better than simply associating all relative distances beyond a certain threshold with the same embedding.\n",
        "Unusually, I won't explain the mathematics behind the logarithmic function used, I think that's boring, and I don't see the point in doing so. Nor did the authors of the paper do so. So leave it alone, but understand its role.\n",
        "\n",
        "**Analysis**\n",
        "* This approach clearly focuses on reducing the number of parameters while maintaining good performance compared to their predecessors, and this was logical, given the results of experiments conducted on their predecessors' method, for example modifying the matrix of values did not give an improvement.\n",
        "* This approach uses embeddings that can be learned during training. This allows the position information to adapt to the task at hand, but it also makes the model vulnerable to overfitting.\n",
        "* Is this approach bounded or unbounded (i.e. can it generalize to arbitrary lengths)? Actually no, it is true that this approach can work with arbitrary lengths except that it clips after a certain distance (after a certain distance it becomes distance-aware), giving all distances the same embedding after a certain distance.\n",
        "\n",
        "\n",
        "**Ref**\n",
        "* [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)."
      ],
      "metadata": {
        "id": "faTb0s3Lszh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import List\n",
        "from torch.nn import functional as F\n",
        "\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1UYT0azJ_w-",
        "outputId": "e4659557-94e5-48a1-dcd4-80de302eb14a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f69e8151450>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RelativePositionBias(nn.Module):\n",
        "    \"\"\"\n",
        "    Translate relative position to a bucket number for relative attention.\n",
        "\n",
        "    The relative position is defined as memory_position - query_position, i.e.\n",
        "    the distance in tokens from the attending position to the attended-to\n",
        "    position. If bidirectional=False, then positive relative positions are\n",
        "    invalid.\n",
        "\n",
        "    We use smaller buckets for small absolute relative_position and larger buckets\n",
        "    for larger absolute relative_positions. All relative positions >=max_distance\n",
        "    map to the same bucket. All relative positions <=-max_distance map to the\n",
        "    same bucket. This should allow for more graceful generalization to longer\n",
        "    sequences than the model has been trained on.\n",
        "\n",
        "    Args:\n",
        "        bidirectional (bool): Whether the attention is bidirectional.\n",
        "        num_buckets (int): Number of buckets.\n",
        "        max_distance (int): Maximum distance for relative positions.\n",
        "        num_heads (int): Number of attention heads.\n",
        "\n",
        "    # REFRANCE: https://github.com/tensorflow/mesh/blob/0cb87fe07da627bf0b7e60475d59f95ed6b5be3d/mesh_tensorflow/transformer/transformer_layers.py#L593\n",
        "    \"\"\"\n",
        "    def __init__(self, bidirectional=True, num_buckets=32, max_distance=128, num_heads=8):\n",
        "        super(RelativePositionBias, self).__init__()\n",
        "        self.bidirectional = bidirectional\n",
        "        self.num_buckets = num_buckets\n",
        "        self.max_distance = max_distance\n",
        "        self.num_heads = num_heads\n",
        "        self.relative_attention_bias = nn.Embedding(self.num_buckets, self.num_heads)\n",
        "\n",
        "    @staticmethod\n",
        "    def _relative_position_bucket(relative_position, bidirectional=True, num_buckets=32, max_distance=128):\n",
        "        \"\"\"\n",
        "        Translate relative position to a bucket number.\n",
        "\n",
        "        Args:\n",
        "            relative_position (torch.Tensor): Relative position tensor.\n",
        "            bidirectional (bool): Whether the attention is bidirectional.\n",
        "            num_buckets (int): Number of buckets.\n",
        "            max_distance (int): Maximum distance for relative positions.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Bucket number tensor.\n",
        "        \"\"\"\n",
        "        ret = torch.ones_like(relative_position, dtype=torch.long)\n",
        "\n",
        "        # Handle bidirectional\n",
        "        if bidirectional:\n",
        "            num_buckets //= 2\n",
        "            ret += (relative_position < 0).to(torch.long) * num_buckets\n",
        "            relative_position = relative_position.abs()\n",
        "\n",
        "        max_exact = num_buckets // 2\n",
        "        is_small = (relative_position < max_exact)\n",
        "\n",
        "        # Compute val_if_large\n",
        "        val_if_large = max_exact + (\n",
        "            torch.log(relative_position.float() / max_exact) /\n",
        "            torch.log(torch.tensor(max_distance / max_exact).float()) * (num_buckets - max_exact)\n",
        "        ).to(torch.long)\n",
        "\n",
        "        # Clamp val_if_large\n",
        "        val_if_large = torch.clamp(val_if_large, max=num_buckets - 1)\n",
        "\n",
        "        # Update ret based on is_small\n",
        "        ret += torch.where(is_small, relative_position, val_if_large)\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def compute_bias(self, qlen, klen):\n",
        "        \"\"\"\n",
        "        Compute binned relative position bias.\n",
        "\n",
        "        Args:\n",
        "            qlen (int): Length of the query sequence.\n",
        "            klen (int): Length of the key sequence.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Relative position bias tensor.\n",
        "        \"\"\"\n",
        "        # Create context and memory positions\n",
        "        context_position = torch.arange(0, qlen, dtype=torch.long, device=self.relative_attention_bias.weight.device)[:, None]\n",
        "        memory_position = torch.arange(0, klen, dtype=torch.long, device=self.relative_attention_bias.weight.device)[None, :]\n",
        "\n",
        "        # Compute relative position\n",
        "        relative_position = memory_position - context_position\n",
        "\n",
        "        # Compute relative position bucket\n",
        "        rp_bucket = self._relative_position_bucket(\n",
        "            relative_position,\n",
        "            bidirectional=self.bidirectional,\n",
        "            num_buckets=self.num_buckets,\n",
        "            max_distance=self.max_distance\n",
        "        )\n",
        "        rp_bucket = rp_bucket.to(self.relative_attention_bias.weight.device)\n",
        "\n",
        "        # Get values from the embedding\n",
        "        values = self.relative_attention_bias(rp_bucket)\n",
        "        values = values.permute([2, 0, 1]).unsqueeze(0)\n",
        "\n",
        "        return values\n",
        "\n",
        "    def forward(self, qlen, klen):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "\n",
        "        Args:\n",
        "            qlen (int): Length of the query sequence.\n",
        "            klen (int): Length of the key sequence.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Relative position bias tensor.\n",
        "        \"\"\"\n",
        "        return self.compute_bias(qlen, klen)\n"
      ],
      "metadata": {
        "id": "Aqj3pYebI1_I"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "def test_relative_position_bias():\n",
        "    # Instantiate the RelativePositionBias module\n",
        "    num_buckets = 32\n",
        "    max_distance = 128\n",
        "    num_heads = 2\n",
        "    relative_position_bias = RelativePositionBias(num_buckets=num_buckets, max_distance=max_distance, num_heads=num_heads)\n",
        "\n",
        "    # Example input sequence lengths (can be adjusted based on your data)\n",
        "    qlen = 5\n",
        "    klen = 5\n",
        "\n",
        "    # Compute relative position biases\n",
        "    biases = relative_position_bias(qlen, klen)\n",
        "\n",
        "    # Print the computed biases\n",
        "    print(\"Computed biases shape:\", biases.shape)\n",
        "    print(biases[:,0].shape)\n",
        "    #print(\"Computed biases:\\n\", biases)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_relative_position_bias()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6gWLaRYSoIH",
        "outputId": "333209dc-50fb-404a-90bc-5e56ae65200f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computed biases shape: torch.Size([1, 2, 5, 5])\n",
            "torch.Size([1, 5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "    \"\"\"\n",
        "    Relation-aware attention head implementation.\n",
        "\n",
        "    Args:\n",
        "        hidden_size (int): Hidden size for the model (embedding dimension).\n",
        "        head_dim (int): Dimensionality of the attention head.\n",
        "\n",
        "    Attributes:\n",
        "        query_weights (nn.Linear): Linear layer for query projection.\n",
        "        key_weights (nn.Linear): Linear layer for key projection.\n",
        "        value_weights (nn.Linear): Linear layer for value projection.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size, head_dim):\n",
        "        \"\"\"\n",
        "        Initializes the AttentionHead.\n",
        "\n",
        "        Args:\n",
        "            hidden_size (int): Hidden size for the model (embedding dimension).\n",
        "            head_dim (int): Dimensionality of the attention head.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.head_dim = head_dim\n",
        "        self.query_weights: nn.Linear = nn.Linear(hidden_size, head_dim)\n",
        "        self.key_weights: nn.Linear = nn.Linear(hidden_size, head_dim)\n",
        "        self.value_weights: nn.Linear = nn.Linear(hidden_size, head_dim)\n",
        "\n",
        "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,\n",
        "                 relative_biases:torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Applies attention mechanism to the input query, key, and value tensors.\n",
        "\n",
        "        Args:\n",
        "            query (torch.Tensor): Query tensor.\n",
        "            key (torch.Tensor): Key tensor.\n",
        "            value (torch.Tensor): Value tensor.\n",
        "            mask (torch.Tensor): Optional mask tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Updated value embeddings after applying attention mechanism.\n",
        "        \"\"\"\n",
        "        query: torch.Tensor = self.query_weights(query)\n",
        "        key: torch.Tensor = self.key_weights(key)\n",
        "        value: torch.Tensor = self.value_weights(value)\n",
        "\n",
        "        att_scores: torch.Tensor = (torch.matmul(query, key.transpose(1, 2)) + relative_biases) / self.head_dim ** 0.5\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask.to(torch.int)\n",
        "            att_scores: torch.Tensor = att_scores.masked_fill(mask.unsqueeze(1) == 0, -1e9)\n",
        "\n",
        "        att_weights: torch.Tensor = F.softmax(att_scores, dim=-1)\n",
        "        n_value: torch.Tensor = torch.matmul(att_weights, value)\n",
        "\n",
        "        return n_value\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head attention layer implementation.\n",
        "\n",
        "    Args:\n",
        "        hidden_size (int): Hidden size for the model (embedding dimension).\n",
        "        num_heads (int): Number of attention heads.\n",
        "\n",
        "    Attributes:\n",
        "        hidden_size (int): Hidden size for the model (embedding dimension).\n",
        "        num_heads (int): Number of attention heads.\n",
        "        head_dim (int): Dimensionality of each attention head.\n",
        "        attention_heads (nn.ModuleList): List of AttentionHead layers.\n",
        "        fc (nn.Linear): Fully connected layer for final projection.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size, num_heads):\n",
        "        \"\"\"\n",
        "        Initializes the MultiHeadAttention layer.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.hidden_size: int = hidden_size\n",
        "        self.num_heads: int = num_heads\n",
        "        self.head_dim: int = hidden_size // num_heads\n",
        "        self.attention_heads: nn.ModuleList = nn.ModuleList([AttentionHead(self.hidden_size, self.head_dim) for head_num in range(self.num_heads)])\n",
        "        self.fc: nn.Linear = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, relative_position_bias: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Applies multi-head attention mechanism to the input query, key, and value tensors.\n",
        "\n",
        "        Args:\n",
        "            query (torch.Tensor): Query tensor.\n",
        "            key (torch.Tensor): Key tensor.\n",
        "            value (torch.Tensor): Value tensor.\n",
        "            mask (torch.Tensor): Optional mask tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Updated hidden state after applying multi-head attention mechanism.\n",
        "        \"\"\"\n",
        "        attention_outputs: List[torch.Tensor] = [attention_head(query, key, value, mask=mask, relative_biases=relative_position_bias[:,i]) for i, attention_head in enumerate(self.attention_heads)]\n",
        "        hidden_state: torch.Tensor = torch.cat(attention_outputs, dim=-1)\n",
        "        hidden_state: torch.Tensor = self.fc(hidden_state)\n",
        "        return hidden_state"
      ],
      "metadata": {
        "id": "tUVz-8RmZ5oZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define input dimensions\n",
        "batch_size = 2\n",
        "seq_len = 5\n",
        "hidden_size = 16\n",
        "num_heads = 8\n",
        "num_buckets = 32\n",
        "max_distance = 128\n",
        "\n",
        "# Compute relative position biases\n",
        "relative_position_bias = RelativePositionBias(num_buckets=num_buckets, max_distance=max_distance, num_heads=num_heads)\n",
        "biases = relative_position_bias(seq_len, seq_len)\n",
        "\n",
        "# Create random input tensors\n",
        "x = torch.randn(batch_size, seq_len, hidden_size)\n",
        "\n",
        "# Instantiate the MultiHeadAttention module\n",
        "multihead_attention = MultiHeadAttention(hidden_size=hidden_size, num_heads=num_heads)\n",
        "\n",
        "# Forward pass\n",
        "output = multihead_attention(x, x, x, biases)\n",
        "\n",
        "# Print the output shape\n",
        "print(\"Output Shape:\", output.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJD71t7iG-uv",
        "outputId": "0863e0d5-f033-4030-96d0-4cbc9a53df2d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output Shape: torch.Size([2, 5, 16])\n"
          ]
        }
      ]
    }
  ]
}